{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7088cff4-388b-4004-ac3f-2a1562d7482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from model.model_lora import *\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_lora import save_lora, apply_lora\n",
    "from trainer.trainer_utils import setup_seed\n",
    "\n",
    "from trainer.trainer_utils import get_lr, Logger, is_main_process, lm_checkpoint, init_distributed_mode, setup_seed, init_model, SkipBatchSampler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b38796f-ca9b-4014-9068-f6b8b620c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOC = '/root/xlcoder/MiniMind2-Small/dataset'\n",
    "INSTR_LOC = '/root/xlcoder/MiniMind2-Small/hands-on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9661275-99c8-40f3-8a49-4b36b488174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_model(args):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "#     if 'model' in args.load_from:\n",
    "#         model = MiniMindForCausalLM(MiniMindConfig(\n",
    "#             hidden_size=args.hidden_size,\n",
    "#             num_hidden_layers=args.num_hidden_layers,\n",
    "#             use_moe=bool(args.use_moe),\n",
    "#             inference_rope_scaling=args.inference_rope_scaling\n",
    "#         ))\n",
    "#         moe_suffix = '_moe' if args.use_moe else ''\n",
    "#         ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'\n",
    "#         model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)\n",
    "#         if args.lora_weight != 'None':\n",
    "#             apply_lora(model)\n",
    "#             load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')\n",
    "#     else:\n",
    "#         model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)\n",
    "#     print(f'MiniMind模型参数: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')\n",
    "#     return model.eval().to(args.device), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37580b47-c1f1-4479-bb6c-873b3bd67845",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_LOC, 'bbc_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb959ea5-c00a-4046-acfa-f38eb0dec0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationSFTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MiniMind-compatible SFT Dataset for text classification\n",
    "    Returns: X, Y, loss_mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        max_length=1024,\n",
    "        use_instruction=True,\n",
    "        use_title=True,\n",
    "        instruction_position=\"head\",  # \"head\" | \"middle\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = df\n",
    "\n",
    "        self.instruction = 'Classify the following passage into one of the categories: business, entertainment, politics, sport, tech.'\n",
    "\n",
    "        self.use_instruction = use_instruction\n",
    "        self.use_title = use_title\n",
    "        self.instruction_position = instruction_position\n",
    "\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def format_example(self, ex):\n",
    "        parts = []\n",
    "\n",
    "        instruction = self.instruction\n",
    "        title = ex.title\n",
    "        content = ex.content\n",
    "        label = ex.category\n",
    "\n",
    "        if self.use_instruction and self.instruction_position == \"head\":\n",
    "            parts.append(instruction)\n",
    "            parts.append(\"\")\n",
    "\n",
    "        if self.use_title and title:\n",
    "            parts.append(\"Title:\")\n",
    "            parts.append(title)\n",
    "            parts.append(\"\")\n",
    "\n",
    "        parts.append(\"Text:\")\n",
    "        parts.append(content)\n",
    "        parts.append(\"\")\n",
    "\n",
    "        if self.use_instruction and self.instruction_position == \"middle\":\n",
    "            parts.append(instruction)\n",
    "            parts.append(\"\")\n",
    "\n",
    "        parts.append(\"Label:\")\n",
    "\n",
    "        prompt = \"\\n\".join(parts)\n",
    "        return prompt, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.samples.iloc[idx]\n",
    "        prompt, label = self.format_example(ex)\n",
    "\n",
    "        # 注意：label 前加空格，避免 tokenizer 粘连\n",
    "        full_text = prompt + \" \" + label\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding.input_ids.squeeze(0)\n",
    "        attention_mask = encoding.attention_mask.squeeze(0)\n",
    "\n",
    "        # -------- 构造 loss_mask（核心区别）--------\n",
    "        # 只在 label 部分计算 loss\n",
    "        prompt_ids = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        ).input_ids\n",
    "\n",
    "        loss_mask = torch.zeros_like(input_ids)\n",
    "        start = len(prompt_ids)\n",
    "\n",
    "        # 对 label token 打 1\n",
    "        loss_mask[start: start + (attention_mask[start:].sum())] = 1\n",
    "\n",
    "        # -------- 构造 MiniMind 所需的 X, Y --------\n",
    "        X = input_ids[:-1]\n",
    "        Y = input_ids[1:]\n",
    "        loss_mask = loss_mask[1:]\n",
    "\n",
    "        return X, Y, loss_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b5f0ae-0c8f-4689-9da1-90cb61cf2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, start_step=0, wandb=None):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            res = model(X)\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())\n",
    "\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss += res.aux_loss\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_loss = loss.item() * args.accumulation_steps\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60\n",
    "            \n",
    "            Logger(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:')\n",
    "            \n",
    "            if wandb: wandb.log({\"loss\": current_loss, \"lr\": current_lr, \"epoch_Time\": eta_min})\n",
    "\n",
    "        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():\n",
    "            model.eval()\n",
    "            moe_suffix = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth'\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            state_dict = {k: v.half().cpu() for k, v in state_dict.items()}\n",
    "            torch.save(state_dict, ckp)\n",
    "            lm_checkpoint(lm_config, weight=args.save_weight, model=model, optimizer=optimizer, \n",
    "                         epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints', scaler=scaler)\n",
    "            model.train()\n",
    "            del state_dict\n",
    "\n",
    "        del X, Y, loss_mask, res, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c3a386-56ca-42f5-910b-a2932434b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'load_from': '../model',\n",
    "    'save_dir': '../out',\n",
    "    'save_weight': 'en_text_cls_pf',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 5e-7,\n",
    "    'device': \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'dtype': 'bfloat16',\n",
    "    'num_workers': 1,\n",
    "    'accumulation_steps': 1,\n",
    "    'grad_clip': 1.0,\n",
    "    'log_interval': 100,\n",
    "    'save_interval': 100,\n",
    "    'hidden_size': 512,\n",
    "    'num_hidden_layers': 8,\n",
    "    'max_seq_len': 4096,\n",
    "    'use_moe': 0,\n",
    "    'from_weight': 'en_pretrain',\n",
    "    'from_resume': 1,\n",
    "    'use_wandb': 0,\n",
    "    'wandb_project': \"MiniMind-Classification-SFT\"\n",
    "}\n",
    "args = json.loads(json.dumps(args), object_hook=lambda d: SimpleNamespace(**d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08adaaa-3bfe-4eaf-b15c-1986e3592ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所加载Model可训练参数：25.830 百万\n",
      "Epoch:[1/10](100/112) loss:3.332413 lr:0.000000491206 epoch_Time:0.0min:\n",
      "Epoch:[1/10](111/112) loss:3.489002 lr:0.000000489182 epoch_Time:0.0min:\n",
      "Epoch:[2/10](100/112) loss:1.944174 lr:0.000000461376 epoch_Time:0.0min:\n",
      "Epoch:[2/10](111/112) loss:1.978017 lr:0.000000457399 epoch_Time:0.0min:\n",
      "Epoch:[3/10](100/112) loss:1.160428 lr:0.000000413303 epoch_Time:0.0min:\n",
      "Epoch:[3/10](111/112) loss:1.287139 lr:0.000000407762 epoch_Time:0.0min:\n",
      "Epoch:[4/10](100/112) loss:0.632069 lr:0.000000351691 epoch_Time:0.0min:\n",
      "Epoch:[4/10](111/112) loss:0.817133 lr:0.000000345129 epoch_Time:0.0min:\n",
      "Epoch:[5/10](100/112) loss:0.886273 lr:0.000000282572 epoch_Time:0.0min:\n",
      "Epoch:[5/10](111/112) loss:0.754163 lr:0.000000275631 epoch_Time:0.0min:\n",
      "Epoch:[6/10](100/112) loss:0.623587 lr:0.000000212712 epoch_Time:0.0min:\n",
      "Epoch:[6/10](111/112) loss:0.566902 lr:0.000000206072 epoch_Time:0.0min:\n",
      "Epoch:[7/10](100/112) loss:0.682802 lr:0.000000148949 epoch_Time:0.0min:\n",
      "Epoch:[7/10](111/112) loss:0.623968 lr:0.000000143259 epoch_Time:0.0min:\n",
      "Epoch:[8/10](100/112) loss:0.501072 lr:0.000000097525 epoch_Time:0.0min:\n",
      "Epoch:[8/10](111/112) loss:0.580782 lr:0.000000093343 epoch_Time:0.0min:\n",
      "Epoch:[9/10](100/112) loss:0.476408 lr:0.000000063473 epoch_Time:0.0min:\n",
      "Epoch:[9/10](111/112) loss:0.596167 lr:0.000000061208 epoch_Time:0.0min:\n",
      "Epoch:[10/10](100/112) loss:0.581180 lr:0.000000050127 epoch_Time:0.0min:\n",
      "Epoch:[10/10](111/112) loss:0.485588 lr:0.000000050001 epoch_Time:0.0min:\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. 初始化环境和随机种子 ==========\n",
    "local_rank = init_distributed_mode()\n",
    "if dist.is_initialized(): args.device = f\"cuda:{local_rank}\"\n",
    "setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))\n",
    "\n",
    "# ========== 2. 配置目录、模型参数、检查ckp ==========\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))\n",
    "ckp_data = lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints') if args.from_resume==1 else None\n",
    "\n",
    "# ========== 3. 设置混合精度 ==========\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "dtype = torch.bfloat16 if args.dtype == \"bfloat16\" else torch.float16\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast(dtype=dtype)\n",
    "\n",
    "# ========== 4. 配wandb ==========\n",
    "wandb = None\n",
    "if args.use_wandb and is_main_process():\n",
    "    import swanlab as wandb\n",
    "    wandb_id = ckp_data.get('wandb_id') if ckp_data else None\n",
    "    resume = 'must' if wandb_id else None\n",
    "    wandb_run_name = f\"MiniMind-Full-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "    wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)\n",
    "\n",
    "# ========== 5. 定义模型、数据、优化器 ==========\n",
    "model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)\n",
    "\n",
    "# For Partial Freeze only\n",
    "for p in model.model.embed_tokens.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "layers = model.model.layers\n",
    "freeze_n = len(layers) // 2\n",
    "for i in range(freeze_n):\n",
    "    for p in layers[i].parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "train_ds =  ClassificationSFTDataset(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    max_length=args.max_seq_len,\n",
    "    use_instruction=True,\n",
    "    use_title=True,\n",
    "    instruction_position=\"head\"\n",
    ")\n",
    "train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# ========== 6. 从ckp恢复状态 ==========\n",
    "start_epoch, start_step = 0, 0\n",
    "if ckp_data:\n",
    "    model.load_state_dict(ckp_data['model'])\n",
    "    optimizer.load_state_dict(ckp_data['optimizer'])\n",
    "    scaler.load_state_dict(ckp_data['scaler'])\n",
    "    start_epoch = ckp_data['epoch']\n",
    "    start_step = ckp_data.get('step', 0)\n",
    "\n",
    "# ========== 7. DDP包模型 ==========\n",
    "if dist.is_initialized():\n",
    "    model._ddp_params_and_buffers_to_ignore = {\"freqs_cos\", \"freqs_sin\"}\n",
    "    model = DistributedDataParallel(model, device_ids=[local_rank])\n",
    "\n",
    "# ========== 8. 开始训练 ==========\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    train_sampler and train_sampler.set_epoch(epoch)\n",
    "    if epoch == start_epoch and start_step > 0: # 第一个epoch且存在检查点\n",
    "        batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)\n",
    "        loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "        Logger(f'Epoch [{epoch + 1}/{args.epochs}]: 跳过前{start_step}个step，从step {start_step + 1}开始')\n",
    "        train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)\n",
    "    else: # 默认从头开始\n",
    "        loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "        train_epoch(epoch, loader, len(loader), 0, wandb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73680f7-3cdc-4059-8ba8-5240623d028c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
