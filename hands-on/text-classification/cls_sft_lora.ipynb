{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7088cff4-388b-4004-ac3f-2a1562d7482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from model.model_lora import *\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_lora import save_lora, apply_lora\n",
    "from trainer.trainer_utils import setup_seed\n",
    "\n",
    "from trainer.trainer_utils import get_lr, Logger, is_main_process, lm_checkpoint, init_distributed_mode, setup_seed, init_model, SkipBatchSampler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b38796f-ca9b-4014-9068-f6b8b620c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOC = '/root/xlcoder/MiniMind2-Small/dataset'\n",
    "INSTR_LOC = '/root/xlcoder/MiniMind2-Small/hands-on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9661275-99c8-40f3-8a49-4b36b488174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_model(args):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "#     if 'model' in args.load_from:\n",
    "#         model = MiniMindForCausalLM(MiniMindConfig(\n",
    "#             hidden_size=args.hidden_size,\n",
    "#             num_hidden_layers=args.num_hidden_layers,\n",
    "#             use_moe=bool(args.use_moe),\n",
    "#             inference_rope_scaling=args.inference_rope_scaling\n",
    "#         ))\n",
    "#         moe_suffix = '_moe' if args.use_moe else ''\n",
    "#         ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'\n",
    "#         model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)\n",
    "#         if args.lora_weight != 'None':\n",
    "#             apply_lora(model)\n",
    "#             load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')\n",
    "#     else:\n",
    "#         model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)\n",
    "#     print(f'MiniMind模型参数: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')\n",
    "#     return model.eval().to(args.device), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37580b47-c1f1-4479-bb6c-873b3bd67845",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_LOC, 'bbc_train_std.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb959ea5-c00a-4046-acfa-f38eb0dec0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationSFTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MiniMind-compatible SFT Dataset for text classification\n",
    "    Returns: X, Y, loss_mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        max_length=1024,\n",
    "        use_instruction=True,\n",
    "        use_title=True,\n",
    "        instruction_position=\"head\",  # \"head\" | \"middle\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = df\n",
    "\n",
    "        self.instruction = 'Classify the following passage into one of the categories: <CLS_B>, <CLS_E>, <CLS_P>, <CLS_S>, <CLS_T>.'\n",
    "\n",
    "        self.use_instruction = use_instruction\n",
    "        self.use_title = use_title\n",
    "        self.instruction_position = instruction_position\n",
    "\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def format_example(self, ex):\n",
    "        parts = []\n",
    "\n",
    "        instruction = self.instruction\n",
    "        title = ex.title\n",
    "        content = ex.content\n",
    "        label = ex.category\n",
    "\n",
    "        if self.use_instruction and self.instruction_position == \"head\":\n",
    "            parts.append(instruction)\n",
    "            parts.append(\"\")\n",
    "\n",
    "        if self.use_title and title:\n",
    "            parts.append(\"Title:\")\n",
    "            parts.append(title)\n",
    "            parts.append(\"\")\n",
    "\n",
    "        parts.append(\"Text:\")\n",
    "        parts.append(content)\n",
    "        parts.append(\"\")\n",
    "\n",
    "        if self.use_instruction and self.instruction_position == \"middle\":\n",
    "            parts.append(instruction)\n",
    "            parts.append(\"\")\n",
    "\n",
    "        parts.append(\"Label:\")\n",
    "\n",
    "        prompt = \"\\n\".join(parts)\n",
    "        return prompt, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.samples.iloc[idx]\n",
    "        prompt, label = self.format_example(ex)\n",
    "\n",
    "        # 注意：label 前加空格，避免 tokenizer 粘连\n",
    "        full_text = prompt + \" \" + label\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding.input_ids.squeeze(0)\n",
    "        attention_mask = encoding.attention_mask.squeeze(0)\n",
    "\n",
    "        # -------- 构造 loss_mask（核心区别）--------\n",
    "        # 只在 label 部分计算 loss\n",
    "        prompt_ids = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        ).input_ids\n",
    "\n",
    "        loss_mask = torch.zeros_like(input_ids)\n",
    "        start = len(prompt_ids)\n",
    "\n",
    "        # 对 label token 打 1\n",
    "        loss_mask[start: start + (attention_mask[start:].sum())] = 1\n",
    "\n",
    "        # -------- 构造 MiniMind 所需的 X, Y --------\n",
    "        X = input_ids[:-1]\n",
    "        Y = input_ids[1:]\n",
    "        loss_mask = loss_mask[1:]\n",
    "\n",
    "        return X, Y, loss_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b5f0ae-0c8f-4689-9da1-90cb61cf2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, loader, iters, start_step=0, wandb=None):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            res = model(X)\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())\n",
    "\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss += res.aux_loss\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_loss = loss.item() * args.accumulation_steps\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60\n",
    "            \n",
    "            Logger(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:')\n",
    "            \n",
    "            if wandb: wandb.log({\"loss\": current_loss, \"lr\": current_lr, \"epoch_Time\": eta_min})\n",
    "\n",
    "        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():\n",
    "            model.eval()\n",
    "            moe_suffix = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth'\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            state_dict = {k: v.half().cpu() for k, v in state_dict.items()}\n",
    "            torch.save(state_dict, ckp)\n",
    "            lm_checkpoint(lm_config, weight=args.save_weight, model=model, optimizer=optimizer, \n",
    "                         epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints', scaler=scaler)\n",
    "            model.train()\n",
    "            del state_dict\n",
    "\n",
    "        del X, Y, loss_mask, res, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a51daa7-40cc-4925-a7a5-87ce6b853e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_lora(epoch, loader, iters, lora_params, start_step=0, wandb=None):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with autocast_ctx:\n",
    "            res = model(X)\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())\n",
    "\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss += res.aux_loss\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if step % args.log_interval == 0 or step == iters - 1:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_loss = loss.item() * args.accumulation_steps\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60\n",
    "            \n",
    "            Logger(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:')\n",
    "            \n",
    "            if wandb: wandb.log({\"loss\": current_loss, \"lr\": current_lr, \"epoch_Time\": eta_min})\n",
    "\n",
    "        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():\n",
    "            model.eval()\n",
    "            lora_save_path = f'{args.save_dir}/lora/{args.save_weight}_{lm_config.hidden_size}.pth'\n",
    "            # LoRA只保存LoRA权重\n",
    "            save_lora(model, lora_save_path)\n",
    "            lm_checkpoint(lm_config, weight=args.save_weight, model=model, optimizer=optimizer, scaler=scaler, epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints')\n",
    "            model.train()\n",
    "\n",
    "        del X, Y, loss_mask, res, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c3a386-56ca-42f5-910b-a2932434b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'load_from': '../model',\n",
    "    'save_dir': '../out',\n",
    "    'save_weight': 'en_text_cls_logits',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 5e-7,\n",
    "    'device': \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'dtype': 'bfloat16',\n",
    "    'num_workers': 1,\n",
    "    'accumulation_steps': 1,\n",
    "    'grad_clip': 1.0,\n",
    "    'log_interval': 100,\n",
    "    'save_interval': 100,\n",
    "    'hidden_size': 512,\n",
    "    'num_hidden_layers': 8,\n",
    "    'max_seq_len': 4096,\n",
    "    'use_moe': 0,\n",
    "    'from_weight': 'en_pretrain',\n",
    "    'from_resume': 1,\n",
    "    'use_wandb': 0,\n",
    "    'wandb_project': \"MiniMind-Classification-SFT\",\n",
    "    'train_mode': ''\n",
    "}\n",
    "args = json.loads(json.dumps(args), object_hook=lambda d: SimpleNamespace(**d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08adaaa-3bfe-4eaf-b15c-1986e3592ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所加载Model可训练参数：25.830 百万\n",
      "Epoch [5/10]: 跳过前111个step，从step 112开始\n",
      "Epoch:[6/10](100/112) loss:0.782172 lr:0.000000212712 epoch_Time:0.0min:\n",
      "Epoch:[6/10](111/112) loss:0.919683 lr:0.000000206072 epoch_Time:0.0min:\n",
      "Epoch:[7/10](100/112) loss:0.635566 lr:0.000000148949 epoch_Time:0.0min:\n",
      "Epoch:[7/10](111/112) loss:0.736335 lr:0.000000143259 epoch_Time:0.0min:\n",
      "Epoch:[8/10](100/112) loss:0.660283 lr:0.000000097525 epoch_Time:0.0min:\n",
      "Epoch:[8/10](111/112) loss:0.678993 lr:0.000000093343 epoch_Time:0.0min:\n",
      "Epoch:[9/10](100/112) loss:0.677492 lr:0.000000063473 epoch_Time:0.0min:\n",
      "Epoch:[9/10](111/112) loss:0.713626 lr:0.000000061208 epoch_Time:0.0min:\n",
      "Epoch:[10/10](100/112) loss:0.688024 lr:0.000000050127 epoch_Time:0.0min:\n",
      "Epoch:[10/10](111/112) loss:0.568730 lr:0.000000050001 epoch_Time:0.0min:\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. 初始化环境和随机种子 ==========\n",
    "local_rank = init_distributed_mode()\n",
    "if dist.is_initialized(): args.device = f\"cuda:{local_rank}\"\n",
    "setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))\n",
    "\n",
    "# ========== 2. 配置目录、模型参数、检查ckp ==========\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "if args.train_mode == 'lora':\n",
    "    os.makedirs(os.path.join(args.save_dir, 'lora'), exist_ok=True)\n",
    "lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))\n",
    "ckp_data = lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints') if args.from_resume==1 else None\n",
    "\n",
    "# ========== 3. 设置混合精度 ==========\n",
    "device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "dtype = torch.bfloat16 if args.dtype == \"bfloat16\" else torch.float16\n",
    "autocast_ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast(dtype=dtype)\n",
    "\n",
    "# ========== 4. 配wandb ==========\n",
    "wandb = None\n",
    "if args.use_wandb and is_main_process():\n",
    "    import swanlab as wandb\n",
    "    wandb_id = ckp_data.get('wandb_id') if ckp_data else None\n",
    "    resume = 'must' if wandb_id else None\n",
    "    wandb_run_name = f\"MiniMind-LoRA-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "    wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)\n",
    "\n",
    "# ========== 5. 定义模型、数据、优化器 ==========\n",
    "model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)\n",
    "\n",
    "SPECIAL_LABELS = [\n",
    "    \"<CLS_B>\",\n",
    "    \"<CLS_E>\",\n",
    "    \"<CLS_P>\",\n",
    "    \"<CLS_S>\",\n",
    "    \"<CLS_T>\",\n",
    "]\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": SPECIAL_LABELS\n",
    "})\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "if args.train_mode == 'lora':\n",
    "    apply_lora(model, rank=1024)\n",
    "    \n",
    "    # 统计参数\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name)\n",
    "    Logger(f\"LLM 总参数量: {total_params / 1e6:.3f} M\")\n",
    "    Logger(f\"LoRA 参数量: {lora_params_count / 1e6:.3f} M\")\n",
    "    Logger(f\"LoRA 参数占比: {lora_params_count / total_params * 100:.2f}%\")\n",
    "    \n",
    "    # 冻结非LoRA参数，收集LoRA参数\n",
    "    lora_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "            lora_params.append(param)\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "if args.train_mode == 'partial_freeze':\n",
    "    # For Partial Freeze only\n",
    "    for p in model.model.embed_tokens.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    layers = model.model.layers\n",
    "    freeze_n = len(layers) // 2\n",
    "    for i in range(freeze_n):\n",
    "        for p in layers[i].parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "train_ds =  ClassificationSFTDataset(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    max_length=args.max_seq_len,\n",
    "    use_instruction=True,\n",
    "    use_title=True,\n",
    "    instruction_position=\"head\"\n",
    ")\n",
    "train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n",
    "if args.train_mode == 'lora':\n",
    "    optimizer = optim.AdamW(lora_params, lr=args.learning_rate)\n",
    "else:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# ========== 6. 从ckp恢复状态 ==========\n",
    "start_epoch, start_step = 0, 0\n",
    "if ckp_data:\n",
    "    model.load_state_dict(ckp_data['model'])\n",
    "    optimizer.load_state_dict(ckp_data['optimizer'])\n",
    "    scaler.load_state_dict(ckp_data['scaler'])\n",
    "    start_epoch = ckp_data['epoch']\n",
    "    start_step = ckp_data.get('step', 0)\n",
    "\n",
    "# ========== 7. DDP包模型 ==========\n",
    "if dist.is_initialized():\n",
    "    model._ddp_params_and_buffers_to_ignore = {\"freqs_cos\", \"freqs_sin\"}\n",
    "    model = DistributedDataParallel(model, device_ids=[local_rank])\n",
    "\n",
    "# ========== 8. 开始训练 ==========\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    train_sampler and train_sampler.set_epoch(epoch)\n",
    "    if epoch == start_epoch and start_step > 0: # 第一个epoch且存在检查点\n",
    "        batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)\n",
    "        loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "        Logger(f'Epoch [{epoch + 1}/{args.epochs}]: 跳过前{start_step}个step，从step {start_step + 1}开始')\n",
    "        if args.train_mode == 'lora':\n",
    "            train_epoch_lora(epoch, loader, len(loader) + start_step + 1, lora_params, start_step, wandb)\n",
    "        else:\n",
    "            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)\n",
    "    else: # 默认从头开始\n",
    "        loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "        if args.train_mode == 'lora':\n",
    "            train_epoch_lora(epoch, loader, len(loader), lora_params, 0, wandb)\n",
    "        else:\n",
    "            train_epoch(epoch, loader, len(loader), 0, wandb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73680f7-3cdc-4059-8ba8-5240623d028c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
